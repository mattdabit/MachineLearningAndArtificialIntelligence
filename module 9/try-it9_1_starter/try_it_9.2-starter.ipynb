{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try-it 9.2: Predicting Wages\n",
    "\n",
    "This activity is meant to summarize your work with regularized regression models.  You will use your earlier work with data preparation and pipelines together with what you've learned with grid searches to determine an optimal model.  In addition to the prior strategies, this example is an excellent opportunity to utilize the `TransformedTargetRegressor` estimator in scikitlearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "This dataset is loaded from the OpenML resource library.  Originally from census data, the data contains wage and demographic information on 534 individuals. From the dataset documentation [here](https://www.openml.org/d/534)\n",
    "\n",
    "```\n",
    "The Current Population Survey (CPS) is used to supplement census information between census years. These data consist of a random sample of 534 persons from the CPS, with information on wages and other characteristics of the workers, including sex, number of years of education, years of work experience, occupational status, region of residence, and union membership. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.datasets import fetch_openml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "wages = fetch_openml(data_id=534, as_frame=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "wages.frame.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Build regression models to predict `WAGE`.  Incorporate the categorical features and transform the target using a logarithm.  Build `Ridge` models and consider some different amounts of regularization.  \n",
    "\n",
    "After fitting your model, interpret the model and try to understand what features led to higher wages.  Consider using `permutation_importance` that you encountered in module 8.  Discuss your findings in the class forum.\n",
    "\n",
    "For an in depth example discussing the perils of interpreting the coefficients, see the example in scikitlearn examples [here](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import TransformedTargetRegressor, make_column_selector, ColumnTransformer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = wages.frame\n",
    "df.info()\n",
    "df.columns = df.columns.str.lower()\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df.drop('wage', axis=1)\n",
    "y = df['wage']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessing = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(), make_column_selector(dtype_include=\"category\")),\n",
    "     ('num', 'passthrough', make_column_selector(dtype_exclude=\"category\"))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "linreg_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('seq', SequentialFeatureSelector(estimator=LinearRegression())),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'seq__n_features_to_select': [3, 4, 5, 6],\n",
    "    'poly__degree': [2, 3, 4],\n",
    "}\n",
    "\n",
    "grid_linreg = GridSearchCV(estimator=linreg_pipe, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_linreg.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "linreg_train_mse = mean_squared_error(grid_linreg.best_estimator_.predict(X_train), y_train)\n",
    "linreg_test_mse = mean_squared_error(grid_linreg.best_estimator_.predict(X_test), y_test)\n",
    "print(f'Train MSE: {linreg_train_mse}, Test MSE: {linreg_test_mse}, Best Params: {grid_linreg.best_params_}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ridge_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('seq', SequentialFeatureSelector(estimator=LinearRegression())),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'seq__n_features_to_select': [3, 4, 5, 6],\n",
    "    'poly__degree': [2, 3, 4],\n",
    "    'ridge__alpha': [0.01, 0.1, 1, 10],\n",
    "}\n",
    "\n",
    "grid_ridge = GridSearchCV(estimator=ridge_pipe, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_ridge.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ridge_train_mse = mean_squared_error(grid_ridge.best_estimator_.predict(X_train), y_train)\n",
    "ridge_test_mse = mean_squared_error(grid_ridge.best_estimator_.predict(X_test), y_test)\n",
    "print(f'Train MSE: {ridge_train_mse}, Test MSE: {ridge_test_mse}, Best Params: {grid_ridge.best_params_}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('seq', SequentialFeatureSelector(estimator=LinearRegression())),\n",
    "    ('transform_linreg', TransformedTargetRegressor(LinearRegression(), func=np.log, inverse_func=np.exp)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'seq__n_features_to_select': [3, 4, 5, 6],\n",
    "    'poly__degree': [2, 3, 4],\n",
    "}\n",
    "\n",
    "grid_target = GridSearchCV(estimator=target_pipe, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_target.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_train_mse = mean_squared_error(grid_target.best_estimator_.predict(X_train), y_train)\n",
    "target_test_mse = mean_squared_error(grid_target.best_estimator_.predict(X_test), y_test)\n",
    "print(f'Train MSE: {target_train_mse}, Test MSE: {target_test_mse}, Best Params: {grid_target.best_params_}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
