{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4f3ef0fa8eec903",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Self-Study Colab Activity 8.3: Using Validation to Select the Best Combination of Parameters\n",
    "\n",
    "**Expected Time: 60 Minutes**\n",
    "\n",
    "\n",
    "This activity focuses on using a train/test split to select the best hyperparameters for a linear regression model complexity.  You will become familiar with scikit-learn's `train_test_split` function to generate a train/test split and use the results to evaluate the appropriate model complexity.  The datasets used are synthetic so as to allow a comparison with the learned best complexity to that which generated the data.  \n",
    "\n",
    "## Index:\n",
    "\n",
    "- [Problem 1](#Problem-1)\n",
    "- [Problem 2](#Problem-2)\n",
    "- [Problem 3](#Problem-3)\n",
    "- [Problem 4](#Problem-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4816d0d701347e37",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Three Synthetic Datasets\n",
    "\n",
    "Below, polynomial functions of different degrees were created, and noise was added to generate three basic synthetic datasets.  The relationships are then plotted. They are of varying true complexity -- cubic, quadratic, and quintic (polynomials of degree 5).  Your goal is to use cross-validation to determine the appropriate model and examine its mean squared error on a set of validation data. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df = pd.read_csv('module 8/colab_activity8_3_starter/data/synthetic_8.6.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots of the Synthetic Datasets**\n",
    "\n",
    "<img src = 'images/quad.png'/><img src = 'images/quintic.png'/><img src = 'images/cubic.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-51eae9cb036d0cf0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Creating the Train and Test sets\n",
    "\n",
    "\n",
    "The scikit-learn library has a built-in function called `train_test_split` that accepts one or many arrays and returns a randomized split of the data.  Use the `train_test_split` function to split `x` and `y1` into train and test sets.  Set `random_state = 32` and create a test set using 30% of the data.  Assign your results as arrays to `X_train, X_test, y1_train, y1_test` below.  \n",
    "\n",
    "- In anticipation of using `LinearRegression` estimator, make sure your `X_train` and `X_test` are of shapes (70, 1) and (30, 1) respectively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9434424b94dba3c0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "X_train, X_test, y1_train, y1_test = train_test_split(df[['x']], df[['y1']], test_size=30, random_state=32)\n",
    "\n",
    "# Answer check\n",
    "print(df['x'].shape, X_train.shape, X_test.shape)\n",
    "print(X_train.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de81ef2bbaab88ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 2\n",
    "\n",
    "\n",
    "Use the `train_test_split` function to create similar splits of `y2` and `y3`.  Use the `random_state = 32` and create a test set using 30% of the data.   Assign your results to `y2_train`, `y2_test`, `y3_train`, and `y3_test` below.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14a80d1dd8e68ebc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "y2_train, y2_test, y3_train, y3_test = train_test_split(df[['y2']], df[['y3']], test_size=30, random_state=32)\n",
    "\n",
    "# Answer check\n",
    "print(df['y2'].shape, y2_train.shape, y2_test.shape)\n",
    "print(y2_train.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d22e18e493c7a842",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "\n",
    "Use a `for` loop to loop over the values from one to 20. For each iteration `i`:\n",
    "\n",
    "- Use `Pipeline` to create a pipeline object. Inside the pipeline object define a a tuple where the first element is a string identifier `pfeat` and the second element is an instance of `PolynomialFeatures` of degree `i` with `include_bias = False`. Inside the pipeline define another tuple where the first element is a string identifier `linreg`, and the second element is an instance of `LinearRegression`. Assign the pipeline object to the variable `pipe`.\n",
    "- Use the `fit` function on `pipe` to train your model on `X_train` and `y1_train`. Assign the result to `train_preds`.\n",
    "- Use the `predict` function on `pipe` to compute your prediction on `X_test`. Assign the result to `test_preds`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y1_train` and `train_preds`. Append your result to the `train_mses` list.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y1_test` and `test_preds`. Append your result to the `test_mses` list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22388d4a41b01c98",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "\n",
    "\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "\n",
    "#for complexity 1 - 20:\n",
    "\n",
    "#create pipeline with PolynomialFeatures and LinearRegression\n",
    "#remember to set include_bias = False\n",
    "\n",
    "#fit pipeline on training data\n",
    "\n",
    "#mse of training data\n",
    "\n",
    "#mse of testing data\n",
    "\n",
    "for i in range(1, 21):\n",
    "    pipe = Pipeline([\n",
    "        ('pfeat', PolynomialFeatures(degree=i, include_bias=False)),\n",
    "        ('linreg', LinearRegression())\n",
    "    ])\n",
    "    pipe.fit(X_train, y1_train)\n",
    "    train_preds = pipe.predict(X_train)\n",
    "    test_preds = pipe.predict(X_test)\n",
    "    train_mses.append(mean_squared_error(y1_train, train_preds))\n",
    "    test_mses.append(mean_squared_error(y1_test, test_preds))\n",
    "\n",
    "best_model_complexity = '2'\n",
    "\n",
    "\n",
    "# Answer check"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to visualize the results of your model fitting.  Note that the data in `y1` were created from a quadratic model originally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'The Complexity that minimized Test Error was: {test_mses.index(min(test_mses)) + 1}')\n",
    "plt.plot(range(1, 21), train_mses, '--o', label='training error')\n",
    "plt.plot(range(1, 21), test_mses, '--o', label='testing error')\n",
    "plt.xticks(range(1, 21), range(1, 21))\n",
    "plt.xlabel('Degree Complexity')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbff2877fa3e10c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Write a function to determine best model complexity\n",
    "\n",
    "\n",
    "\n",
    "Complete the definition of the `simple_cross_validation` function according to the following instructions:\n",
    "\n",
    "\n",
    "Use a `for` loop to loop over the values from one to 20. For each iteration `i`:\n",
    "\n",
    "- Use `Pipeline` to create a pipeline object. Inside the pipeline object define a a tuple where the first element is a string identifier `pfeat` and the second element is an instance of `PolynomialFeatures` of degree `i` with `include_bias = False`. Inside the pipeline define another tuple where the first element is a string identifier `linreg`, and the second element is an instance of `LinearRegression`. Assign the pipeline object to the variable `pipe`.\n",
    "- Use the `fit` function on `pipe` to train your model on `X_train` and `y_train`. \n",
    "- Use the `predict` function on `pipe` to compute your prediction on `X_test`. Assign the result to `test_preds`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y_test` and `test_preds`. Assign your result to `test_mse`.\n",
    "- Use an `if` statement to check that `test_mse` is less than `best_mse`:\n",
    "    - If the condition is satisifed assign `test_mse` to `best_mse` and `pipe` to `best_pipe`.\n",
    "- Your function should return `best_pipe`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f0fca2c0aa71b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "\n",
    "\n",
    "def simple_cross_validation(X_train, y_train, X_test, y_test):\n",
    "    best_pipe = None  #placeholder for best model\n",
    "    best_mse = np.inf  #set best mse to infinity to begin\n",
    "    for i in range(1, 21):\n",
    "        pipe = Pipeline([\n",
    "            ('pfeat', PolynomialFeatures(degree=i, include_bias=False)),\n",
    "            ('linreg', LinearRegression())\n",
    "        ])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        test_preds = pipe.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, test_preds)\n",
    "        if test_mse < best_mse:\n",
    "            best_mse = test_mse\n",
    "            best_pipe = pipe\n",
    "    return best_pipe\n",
    "\n",
    "\n",
    "best_model = simple_cross_validation(X_train, y2_train, X_test, y2_test)\n",
    "best_model.get_params()  #should be degree = 10"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
